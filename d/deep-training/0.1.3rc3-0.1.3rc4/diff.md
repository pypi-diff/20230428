# Comparing `tmp/deep_training-0.1.3rc3-py3-none-any.whl.zip` & `tmp/deep_training-0.1.3rc4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,160 +1,164 @@
-Zip file size: 278905 bytes, number of entries: 158
--rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 10:40 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      908 b- defN 23-Apr-26 15:23 deep_training/setup.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-19 08:44 deep_training/cv/__init__.py
--rw-rw-rw-  2.0 fat      195 b- defN 23-Mar-19 08:44 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    27373 b- defN 23-Apr-26 15:17 deep_training/data_helper/data_helper.py
--rw-rw-rw-  2.0 fat     5082 b- defN 23-Apr-26 15:23 deep_training/data_helper/data_module.py
--rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-26 15:23 deep_training/data_helper/training_args.py
--rw-rw-rw-  2.0 fat       70 b- defN 23-Mar-19 08:44 deep_training/nlp/__init__.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/__init__.py
--rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/activate.py
--rw-rw-rw-  2.0 fat    13271 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/crf.py
--rw-rw-rw-  2.0 fat     4653 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/handshakingkernel.py
--rw-rw-rw-  2.0 fat      435 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/mask.py
--rw-rw-rw-  2.0 fat     1319 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/mhslayer.py
--rw-rw-rw-  2.0 fat     5911 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/norm.py
--rw-rw-rw-  2.0 fat     1390 b- defN 23-Apr-26 14:08 deep_training/nlp/layers/ppo.py
--rw-rw-rw-  2.0 fat     1220 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/prefix_encoder.py
--rw-rw-rw-  2.0 fat     7259 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/seq_pointer.py
--rw-rw-rw-  2.0 fat     3550 b- defN 23-Mar-19 08:44 deep_training/nlp/layers/w2ner.py
--rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v1/__init__.py
--rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v1/layers.py
--rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v1/utils.py
--rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v2/__init__.py
--rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v2/adalora.py
--rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v2/layers.py
--rw-rw-rw-  2.0 fat     9106 b- defN 23-Apr-15 02:11 deep_training/nlp/layers/lora_v2/utils.py
--rw-rw-rw-  2.0 fat     3662 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/BatchAllTripletLoss.py
--rw-rw-rw-  2.0 fat     3880 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py
--rw-rw-rw-  2.0 fat     8358 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/BatchHardTripletLoss.py
--rw-rw-rw-  2.0 fat     4552 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/BatchSemiHardTripletLoss.py
--rw-rw-rw-  2.0 fat     2255 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/ContrastiveLoss.py
--rw-rw-rw-  2.0 fat     4573 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/ContrastiveTensionLoss.py
--rw-rw-rw-  2.0 fat     1359 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/CosineSimilarityLoss.py
--rw-rw-rw-  2.0 fat      742 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/MSELoss.py
--rw-rw-rw-  2.0 fat     1315 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/MarginMSELoss.py
--rw-rw-rw-  2.0 fat     5302 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/MegaBatchMarginLoss.py
--rw-rw-rw-  2.0 fat     2420 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/MultipleNegativesRankingLoss.py
--rw-rw-rw-  2.0 fat     2905 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/MultipleNegativesSymmetricRankingLoss.py
--rw-rw-rw-  2.0 fat     1863 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/OnlineContrastiveLoss.py
--rw-rw-rw-  2.0 fat     2880 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/SoftmaxLoss.py
--rw-rw-rw-  2.0 fat     2306 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/TripletLoss.py
--rw-rw-rw-  2.0 fat      599 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/__init__.py
--rw-rw-rw-  2.0 fat      661 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/bce_loss.py
--rw-rw-rw-  2.0 fat     1397 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/center_loss.py
--rw-rw-rw-  2.0 fat     1772 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/circle_loss.py
--rw-rw-rw-  2.0 fat     1056 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/contrast.py
--rw-rw-rw-  2.0 fat      619 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/dice_loss.py
--rw-rw-rw-  2.0 fat      710 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/focal_loss.py
--rw-rw-rw-  2.0 fat      882 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/label_smoothing.py
--rw-rw-rw-  2.0 fat      547 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/lm_loss.py
--rw-rw-rw-  2.0 fat     2149 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_arcface.py
--rw-rw-rw-  2.0 fat      962 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_casrel.py
--rw-rw-rw-  2.0 fat     1496 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_cosent.py
--rw-rw-rw-  2.0 fat     1912 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_cosface.py
--rw-rw-rw-  2.0 fat     2223 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_globalpointer.py
--rw-rw-rw-  2.0 fat     6020 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_infonce.py
--rw-rw-rw-  2.0 fat      884 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_kl.py
--rw-rw-rw-  2.0 fat     1270 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_mhslinker.py
--rw-rw-rw-  2.0 fat      617 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_r-drop.py
--rw-rw-rw-  2.0 fat     2656 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_sphereface.py
--rw-rw-rw-  2.0 fat      562 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_splinker.py
--rw-rw-rw-  2.0 fat    10822 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_spn4re.py
--rw-rw-rw-  2.0 fat     5644 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/loss_tplinker.py
--rw-rw-rw-  2.0 fat     2466 b- defN 23-Mar-19 08:44 deep_training/nlp/losses/utils.py
--rw-rw-rw-  2.0 fat       71 b- defN 23-Mar-19 08:44 deep_training/nlp/metrics/__init__.py
--rw-rw-rw-  2.0 fat      655 b- defN 23-Mar-19 08:44 deep_training/nlp/metrics/pointer.py
--rw-rw-rw-  2.0 fat       58 b- defN 23-Mar-19 08:44 deep_training/nlp/models/__init__.py
--rw-rw-rw-  2.0 fat     6826 b- defN 23-Apr-26 14:08 deep_training/nlp/models/casrel.py
--rw-rw-rw-  2.0 fat     5093 b- defN 23-Apr-26 14:08 deep_training/nlp/models/crf_cascad.py
--rw-rw-rw-  2.0 fat     1588 b- defN 23-Apr-26 14:08 deep_training/nlp/models/crf_model.py
--rw-rw-rw-  2.0 fat    12985 b- defN 23-Apr-26 14:08 deep_training/nlp/models/diffcse.py
--rw-rw-rw-  2.0 fat     5395 b- defN 23-Apr-26 14:19 deep_training/nlp/models/esimcse.py
--rw-rw-rw-  2.0 fat     4209 b- defN 23-Apr-26 14:08 deep_training/nlp/models/gec_model.py
--rw-rw-rw-  2.0 fat    10854 b- defN 23-Apr-26 14:08 deep_training/nlp/models/gplinker.py
--rw-rw-rw-  2.0 fat     3814 b- defN 23-Apr-26 14:08 deep_training/nlp/models/infonce.py
--rw-rw-rw-  2.0 fat     2459 b- defN 23-Apr-26 14:08 deep_training/nlp/models/mhs_ner.py
--rw-rw-rw-  2.0 fat     5991 b- defN 23-Apr-26 14:08 deep_training/nlp/models/mhslinker.py
--rw-rw-rw-  2.0 fat     4661 b- defN 23-Apr-26 14:08 deep_training/nlp/models/onerel_model.py
--rw-rw-rw-  2.0 fat     2750 b- defN 23-Apr-26 14:08 deep_training/nlp/models/pointer.py
--rw-rw-rw-  2.0 fat    13392 b- defN 23-Apr-26 14:08 deep_training/nlp/models/prefixtuning.py
--rw-rw-rw-  2.0 fat    15915 b- defN 23-Apr-26 14:08 deep_training/nlp/models/prgc_model.py
--rw-rw-rw-  2.0 fat    16115 b- defN 23-Apr-26 14:08 deep_training/nlp/models/promptbert_cse.py
--rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-26 14:08 deep_training/nlp/models/pure_model.py
--rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-26 14:08 deep_training/nlp/models/simcse.py
--rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-26 14:08 deep_training/nlp/models/span_ner.py
--rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-26 14:08 deep_training/nlp/models/spn4re.py
--rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-26 14:08 deep_training/nlp/models/tplinker.py
--rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-26 14:08 deep_training/nlp/models/tplinkerplus.py
--rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-26 14:08 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26308 b- defN 23-Apr-26 14:19 deep_training/nlp/models/transformer_base.py
--rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-26 14:08 deep_training/nlp/models/tsdae_model.py
--rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-26 14:08 deep_training/nlp/models/w2ner.py
--rw-rw-rw-  2.0 fat    16500 b- defN 23-Mar-25 05:27 deep_training/nlp/models/LLaMA/__init__.py
--rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-19 08:44 deep_training/nlp/models/LLaMA/configuration.py
--rw-rw-rw-  2.0 fat    19183 b- defN 23-Mar-25 05:27 deep_training/nlp/models/LLaMA_parallel/__init__.py
--rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-19 08:44 deep_training/nlp/models/LLaMA_parallel/configuration.py
--rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-25 05:27 deep_training/nlp/models/PaLM/__init__.py
--rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-19 08:44 deep_training/nlp/models/PaLM/configuration.py
--rw-rw-rw-  2.0 fat    60508 b- defN 23-Apr-22 00:37 deep_training/nlp/models/chatglm/__init__.py
--rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-07 11:22 deep_training/nlp/models/chatglm/configuration.py
--rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-02 08:38 deep_training/nlp/models/chatglm/quantization.py
--rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-15 02:44 deep_training/nlp/models/chatglm/tokenization.py
--rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-25 05:27 deep_training/nlp/models/laMDA/__init__.py
--rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-19 08:44 deep_training/nlp/models/laMDA/configuration.py
--rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/__init__.py
--rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v1/__init__.py
--rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v1/configuration.py
--rw-rw-rw-  2.0 fat    13576 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v1/lora_wrapper.py
--rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v2/__init__.py
--rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v2/adalora_model.py
--rw-rw-rw-  2.0 fat    11285 b- defN 23-Apr-26 14:08 deep_training/nlp/models/lora/v2/configuration.py
--rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-22 00:37 deep_training/nlp/models/lora/v2/lora_model.py
--rw-rw-rw-  2.0 fat    10265 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v2/lora_wrapper.py
--rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-15 02:11 deep_training/nlp/models/lora/v2/save_and_load.py
--rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-22 00:37 deep_training/nlp/models/moss/__init__.py
--rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-26 14:08 deep_training/nlp/models/moss/configuration_moss.py
--rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-26 14:08 deep_training/nlp/models/moss/custom_autotune.py
--rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-26 14:08 deep_training/nlp/models/moss/modeling_moss.py
--rw-rw-rw-  2.0 fat    18866 b- defN 23-Apr-26 14:08 deep_training/nlp/models/moss/quantization.py
--rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-26 14:08 deep_training/nlp/models/moss/tokenization_moss.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-22 00:37 deep_training/nlp/models/rlhf/__init__.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-22 00:37 deep_training/nlp/models/rlhf/ppo/__init__.py
--rw-rw-rw-  2.0 fat     2238 b- defN 23-Apr-26 14:08 deep_training/nlp/models/rlhf/ppo/configuration.py
--rw-rw-rw-  2.0 fat     2691 b- defN 23-Apr-26 14:08 deep_training/nlp/models/rlhf/ppo/data_type.py
--rw-rw-rw-  2.0 fat     8064 b- defN 23-Apr-26 14:08 deep_training/nlp/models/rlhf/ppo/ppo.py
--rw-rw-rw-  2.0 fat    16349 b- defN 23-Apr-26 14:08 deep_training/nlp/models/rlhf/ppo/ppo_dataset.py
--rw-rw-rw-  2.0 fat    10496 b- defN 23-Apr-26 14:08 deep_training/nlp/models/rlhf/ppo/utils.py
--rw-rw-rw-  2.0 fat      102 b- defN 23-Mar-19 08:44 deep_training/nlp/models/splinker/__init__.py
--rw-rw-rw-  2.0 fat     2866 b- defN 23-Apr-26 14:08 deep_training/nlp/models/splinker/splinker.py
--rw-rw-rw-  2.0 fat    14478 b- defN 23-Mar-19 08:44 deep_training/nlp/models/t5decoder/__init__.py
--rw-rw-rw-  2.0 fat     6646 b- defN 23-Mar-19 08:44 deep_training/nlp/models/t5encoder/__init__.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Mar-19 08:44 deep_training/nlp/optimizer/__init__.py
--rw-rw-rw-  2.0 fat     5225 b- defN 23-Mar-19 08:44 deep_training/nlp/optimizer/lamb.py
--rw-rw-rw-  2.0 fat       99 b- defN 23-Mar-19 08:44 deep_training/nlp/optimizer/lion/__init__.py
--rw-rw-rw-  2.0 fat     2295 b- defN 23-Mar-19 08:44 deep_training/nlp/optimizer/lion/lion.py
--rw-rw-rw-  2.0 fat     2198 b- defN 23-Mar-19 08:44 deep_training/nlp/optimizer/lion/triton.py
--rw-rw-rw-  2.0 fat     2868 b- defN 23-Mar-19 08:44 deep_training/nlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat     6986 b- defN 23-Apr-26 14:08 deep_training/nlp/utils/__init__.py
--rw-rw-rw-  2.0 fat     6323 b- defN 23-Mar-19 08:44 deep_training/nlp/utils/adversarial.py
--rw-rw-rw-  2.0 fat    15256 b- defN 23-Mar-19 08:44 deep_training/nlp/utils/nlputils.py
--rw-rw-rw-  2.0 fat      795 b- defN 23-Mar-19 08:44 deep_training/nlp/utils/spearman.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/layers/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/losses/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/metrics/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/models/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/optimizer/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-19 08:44 deep_training/tfnlp/utils/__init__.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-19 08:44 deep_training/utils/__init__.py
--rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-19 08:44 deep_training/utils/distributed.py
--rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-19 08:44 deep_training/utils/func.py
--rw-rw-rw-  2.0 fat     5117 b- defN 23-Mar-19 08:44 deep_training/utils/maskedlm.py
--rw-rw-rw-  2.0 fat     7430 b- defN 23-Apr-26 15:33 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      637 b- defN 23-Apr-26 15:52 deep_training-0.1.3rc3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-26 15:52 deep_training-0.1.3rc3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-26 15:52 deep_training-0.1.3rc3.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    15323 b- defN 23-Apr-26 15:52 deep_training-0.1.3rc3.dist-info/RECORD
-158 files, 953403 bytes uncompressed, 254061 bytes compressed:  73.4%
+Zip file size: 284270 bytes, number of entries: 162
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
+-rw-rw-rw-  2.0 fat      908 b- defN 23-Apr-28 00:24 deep_training/setup.py
+-rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
+-rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
+-rw-rw-rw-  2.0 fat    17724 b- defN 23-Apr-28 00:24 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat     5082 b- defN 23-Apr-27 00:33 deep_training/data_helper/data_module.py
+-rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
+-rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
+-rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
+-rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
+-rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
+-rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
+-rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
+-rw-rw-rw-  2.0 fat     1319 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/mhslayer.py
+-rw-rw-rw-  2.0 fat     5911 b- defN 22-Dec-08 00:08 deep_training/nlp/layers/norm.py
+-rw-rw-rw-  2.0 fat     1406 b- defN 23-Apr-28 00:24 deep_training/nlp/layers/ppo.py
+-rw-rw-rw-  2.0 fat     1220 b- defN 22-Jul-21 00:57 deep_training/nlp/layers/prefix_encoder.py
+-rw-rw-rw-  2.0 fat     7259 b- defN 22-Dec-14 02:36 deep_training/nlp/layers/seq_pointer.py
+-rw-rw-rw-  2.0 fat     3550 b- defN 22-Dec-15 00:57 deep_training/nlp/layers/w2ner.py
+-rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/__init__.py
+-rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/layers.py
+-rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/utils.py
+-rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/__init__.py
+-rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/adalora.py
+-rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/layers.py
+-rw-rw-rw-  2.0 fat     9106 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/utils.py
+-rw-rw-rw-  2.0 fat     3662 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchAllTripletLoss.py
+-rw-rw-rw-  2.0 fat     3880 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py
+-rw-rw-rw-  2.0 fat     8358 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardTripletLoss.py
+-rw-rw-rw-  2.0 fat     4552 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchSemiHardTripletLoss.py
+-rw-rw-rw-  2.0 fat     2255 b- defN 22-Nov-18 01:05 deep_training/nlp/losses/ContrastiveLoss.py
+-rw-rw-rw-  2.0 fat     4573 b- defN 22-Nov-16 07:04 deep_training/nlp/losses/ContrastiveTensionLoss.py
+-rw-rw-rw-  2.0 fat     1359 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/CosineSimilarityLoss.py
+-rw-rw-rw-  2.0 fat      742 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/MSELoss.py
+-rw-rw-rw-  2.0 fat     1315 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/MarginMSELoss.py
+-rw-rw-rw-  2.0 fat     5302 b- defN 22-Nov-16 07:11 deep_training/nlp/losses/MegaBatchMarginLoss.py
+-rw-rw-rw-  2.0 fat     2420 b- defN 23-Jan-18 08:08 deep_training/nlp/losses/MultipleNegativesRankingLoss.py
+-rw-rw-rw-  2.0 fat     2905 b- defN 22-Nov-16 07:11 deep_training/nlp/losses/MultipleNegativesSymmetricRankingLoss.py
+-rw-rw-rw-  2.0 fat     1863 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/OnlineContrastiveLoss.py
+-rw-rw-rw-  2.0 fat     2880 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/SoftmaxLoss.py
+-rw-rw-rw-  2.0 fat     2306 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/TripletLoss.py
+-rw-rw-rw-  2.0 fat      599 b- defN 22-Nov-17 00:20 deep_training/nlp/losses/__init__.py
+-rw-rw-rw-  2.0 fat      661 b- defN 22-Dec-02 08:50 deep_training/nlp/losses/bce_loss.py
+-rw-rw-rw-  2.0 fat     1397 b- defN 22-Nov-17 05:38 deep_training/nlp/losses/center_loss.py
+-rw-rw-rw-  2.0 fat     1772 b- defN 22-Nov-17 07:28 deep_training/nlp/losses/circle_loss.py
+-rw-rw-rw-  2.0 fat     1056 b- defN 23-Jan-13 07:26 deep_training/nlp/losses/contrast.py
+-rw-rw-rw-  2.0 fat      619 b- defN 22-Aug-22 01:11 deep_training/nlp/losses/dice_loss.py
+-rw-rw-rw-  2.0 fat      710 b- defN 22-Nov-16 06:49 deep_training/nlp/losses/focal_loss.py
+-rw-rw-rw-  2.0 fat      882 b- defN 22-Aug-22 01:11 deep_training/nlp/losses/label_smoothing.py
+-rw-rw-rw-  2.0 fat      547 b- defN 23-Jan-29 01:07 deep_training/nlp/losses/lm_loss.py
+-rw-rw-rw-  2.0 fat     2149 b- defN 22-Dec-29 06:05 deep_training/nlp/losses/loss_arcface.py
+-rw-rw-rw-  2.0 fat      962 b- defN 22-Dec-05 00:45 deep_training/nlp/losses/loss_casrel.py
+-rw-rw-rw-  2.0 fat     1496 b- defN 22-Dec-21 00:45 deep_training/nlp/losses/loss_cosent.py
+-rw-rw-rw-  2.0 fat     1912 b- defN 22-Dec-29 06:03 deep_training/nlp/losses/loss_cosface.py
+-rw-rw-rw-  2.0 fat     2223 b- defN 22-Dec-08 00:49 deep_training/nlp/losses/loss_globalpointer.py
+-rw-rw-rw-  2.0 fat     6020 b- defN 23-Jan-09 00:29 deep_training/nlp/losses/loss_infonce.py
+-rw-rw-rw-  2.0 fat      884 b- defN 22-Dec-23 00:05 deep_training/nlp/losses/loss_kl.py
+-rw-rw-rw-  2.0 fat     1270 b- defN 23-Mar-14 03:03 deep_training/nlp/losses/loss_mhslinker.py
+-rw-rw-rw-  2.0 fat      617 b- defN 22-Dec-14 08:37 deep_training/nlp/losses/loss_r-drop.py
+-rw-rw-rw-  2.0 fat     2656 b- defN 22-Dec-29 06:04 deep_training/nlp/losses/loss_sphereface.py
+-rw-rw-rw-  2.0 fat      562 b- defN 22-Nov-25 06:44 deep_training/nlp/losses/loss_splinker.py
+-rw-rw-rw-  2.0 fat    10822 b- defN 23-Jan-09 09:00 deep_training/nlp/losses/loss_spn4re.py
+-rw-rw-rw-  2.0 fat     5644 b- defN 22-Dec-12 00:12 deep_training/nlp/losses/loss_tplinker.py
+-rw-rw-rw-  2.0 fat     2466 b- defN 23-Jan-18 09:12 deep_training/nlp/losses/utils.py
+-rw-rw-rw-  2.0 fat       71 b- defN 22-Dec-13 03:17 deep_training/nlp/metrics/__init__.py
+-rw-rw-rw-  2.0 fat      655 b- defN 22-Dec-02 00:22 deep_training/nlp/metrics/pointer.py
+-rw-rw-rw-  2.0 fat       58 b- defN 22-Nov-22 08:00 deep_training/nlp/models/__init__.py
+-rw-rw-rw-  2.0 fat     6826 b- defN 23-Apr-25 03:34 deep_training/nlp/models/casrel.py
+-rw-rw-rw-  2.0 fat     5093 b- defN 23-Apr-25 03:34 deep_training/nlp/models/crf_cascad.py
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-Apr-25 03:34 deep_training/nlp/models/crf_model.py
+-rw-rw-rw-  2.0 fat    12985 b- defN 23-Apr-25 03:34 deep_training/nlp/models/diffcse.py
+-rw-rw-rw-  2.0 fat     5395 b- defN 23-Apr-27 00:33 deep_training/nlp/models/esimcse.py
+-rw-rw-rw-  2.0 fat     4209 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gec_model.py
+-rw-rw-rw-  2.0 fat    10854 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gplinker.py
+-rw-rw-rw-  2.0 fat     3814 b- defN 23-Apr-25 03:34 deep_training/nlp/models/infonce.py
+-rw-rw-rw-  2.0 fat     2459 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhs_ner.py
+-rw-rw-rw-  2.0 fat     5991 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhslinker.py
+-rw-rw-rw-  2.0 fat     4661 b- defN 23-Apr-25 03:34 deep_training/nlp/models/onerel_model.py
+-rw-rw-rw-  2.0 fat     2750 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pointer.py
+-rw-rw-rw-  2.0 fat    13392 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prefixtuning.py
+-rw-rw-rw-  2.0 fat    15915 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prgc_model.py
+-rw-rw-rw-  2.0 fat    16115 b- defN 23-Apr-25 03:34 deep_training/nlp/models/promptbert_cse.py
+-rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
+-rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
+-rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
+-rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
+-rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
+-rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
+-rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
+-rw-rw-rw-  2.0 fat    26308 b- defN 23-Apr-27 00:33 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
+-rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
+-rw-rw-rw-  2.0 fat    16500 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA/__init__.py
+-rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
+-rw-rw-rw-  2.0 fat    19183 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA_parallel/__init__.py
+-rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
+-rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
+-rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
+-rw-rw-rw-  2.0 fat    60508 b- defN 23-Apr-19 01:01 deep_training/nlp/models/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
+-rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
+-rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-17 00:24 deep_training/nlp/models/chatglm/tokenization.py
+-rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
+-rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
+-rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
+-rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
+-rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/configuration.py
+-rw-rw-rw-  2.0 fat    13576 b- defN 23-Apr-12 08:35 deep_training/nlp/models/lora/v1/lora_wrapper.py
+-rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
+-rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
+-rw-rw-rw-  2.0 fat    11285 b- defN 23-Apr-26 08:14 deep_training/nlp/models/lora/v2/configuration.py
+-rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 01:24 deep_training/nlp/models/lora/v2/lora_model.py
+-rw-rw-rw-  2.0 fat    10265 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/lora_wrapper.py
+-rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
+-rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-21 04:29 deep_training/nlp/models/moss/__init__.py
+-rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-23 01:11 deep_training/nlp/models/moss/configuration_moss.py
+-rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-23 01:05 deep_training/nlp/models/moss/custom_autotune.py
+-rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-23 02:08 deep_training/nlp/models/moss/modeling_moss.py
+-rw-rw-rw-  2.0 fat    18750 b- defN 23-Apr-27 02:10 deep_training/nlp/models/moss/quantization.py
+-rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-24 00:32 deep_training/nlp/models/moss/tokenization_moss.py
+-rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
+-rw-rw-rw-  2.0 fat     2866 b- defN 23-Apr-25 03:34 deep_training/nlp/models/splinker/splinker.py
+-rw-rw-rw-  2.0 fat    14478 b- defN 23-Feb-11 09:07 deep_training/nlp/models/t5decoder/__init__.py
+-rw-rw-rw-  2.0 fat     6646 b- defN 23-Feb-09 00:28 deep_training/nlp/models/t5encoder/__init__.py
+-rw-rw-rw-  2.0 fat       56 b- defN 22-Dec-14 08:02 deep_training/nlp/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat     5225 b- defN 23-Mar-08 00:14 deep_training/nlp/optimizer/lamb.py
+-rw-rw-rw-  2.0 fat       99 b- defN 23-Mar-02 05:27 deep_training/nlp/optimizer/lion/__init__.py
+-rw-rw-rw-  2.0 fat     2295 b- defN 23-Mar-02 05:27 deep_training/nlp/optimizer/lion/lion.py
+-rw-rw-rw-  2.0 fat     2198 b- defN 23-Mar-02 05:27 deep_training/nlp/optimizer/lion/triton.py
+-rw-rw-rw-  2.0 fat       79 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/__init__.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/__init__.py
+-rw-rw-rw-  2.0 fat     2238 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/configuration.py
+-rw-rw-rw-  2.0 fat     2691 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/data_type.py
+-rw-rw-rw-  2.0 fat     8063 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/ppo.py
+-rw-rw-rw-  2.0 fat    16348 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/ppo_dataset.py
+-rw-rw-rw-  2.0 fat    10496 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/utils.py
+-rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-27 01:46 deep_training/nlp/rl/ppo_fabric/__init__.py
+-rw-rw-rw-  2.0 fat     8621 b- defN 23-Apr-27 01:57 deep_training/nlp/rl/ppo_fabric/engine.py
+-rw-rw-rw-  2.0 fat     5736 b- defN 23-Apr-28 04:34 deep_training/nlp/rl/ppo_fabric/ppo.py
+-rw-rw-rw-  2.0 fat     5977 b- defN 23-Apr-28 00:24 deep_training/nlp/rl/ppo_fabric/ppo_agent.py
+-rw-rw-rw-  2.0 fat     2868 b- defN 22-Dec-14 08:00 deep_training/nlp/scheduler/__init__.py
+-rw-rw-rw-  2.0 fat     6986 b- defN 23-Apr-26 05:39 deep_training/nlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6323 b- defN 23-Jan-29 01:07 deep_training/nlp/utils/adversarial.py
+-rw-rw-rw-  2.0 fat    15256 b- defN 23-Jan-03 01:54 deep_training/nlp/utils/nlputils.py
+-rw-rw-rw-  2.0 fat      795 b- defN 23-Jan-11 07:02 deep_training/nlp/utils/spearman.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/layers/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/losses/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/metrics/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/models/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
+-rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
+-rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
+-rw-rw-rw-  2.0 fat     7430 b- defN 23-Apr-27 00:33 deep_training/utils/trainer.py
+-rw-rw-rw-  2.0 fat      613 b- defN 23-Apr-28 04:35 deep_training-0.1.3rc4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-28 04:35 deep_training-0.1.3rc4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-28 04:35 deep_training-0.1.3rc4.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    15652 b- defN 23-Apr-28 04:35 deep_training-0.1.3rc4.dist-info/RECORD
+162 files, 964369 bytes uncompressed, 258916 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -354,60 +354,72 @@
 
 Filename: deep_training/nlp/models/moss/quantization.py
 Comment: 
 
 Filename: deep_training/nlp/models/moss/tokenization_moss.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/__init__.py
+Filename: deep_training/nlp/models/splinker/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/__init__.py
+Filename: deep_training/nlp/models/splinker/splinker.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/configuration.py
+Filename: deep_training/nlp/models/t5decoder/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/data_type.py
+Filename: deep_training/nlp/models/t5encoder/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/ppo.py
+Filename: deep_training/nlp/optimizer/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/ppo_dataset.py
+Filename: deep_training/nlp/optimizer/lamb.py
 Comment: 
 
-Filename: deep_training/nlp/models/rlhf/ppo/utils.py
+Filename: deep_training/nlp/optimizer/lion/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/splinker/__init__.py
+Filename: deep_training/nlp/optimizer/lion/lion.py
 Comment: 
 
-Filename: deep_training/nlp/models/splinker/splinker.py
+Filename: deep_training/nlp/optimizer/lion/triton.py
 Comment: 
 
-Filename: deep_training/nlp/models/t5decoder/__init__.py
+Filename: deep_training/nlp/rl/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/models/t5encoder/__init__.py
+Filename: deep_training/nlp/rl/ppo/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/optimizer/__init__.py
+Filename: deep_training/nlp/rl/ppo/configuration.py
 Comment: 
 
-Filename: deep_training/nlp/optimizer/lamb.py
+Filename: deep_training/nlp/rl/ppo/data_type.py
 Comment: 
 
-Filename: deep_training/nlp/optimizer/lion/__init__.py
+Filename: deep_training/nlp/rl/ppo/ppo.py
 Comment: 
 
-Filename: deep_training/nlp/optimizer/lion/lion.py
+Filename: deep_training/nlp/rl/ppo/ppo_dataset.py
 Comment: 
 
-Filename: deep_training/nlp/optimizer/lion/triton.py
+Filename: deep_training/nlp/rl/ppo/utils.py
+Comment: 
+
+Filename: deep_training/nlp/rl/ppo_fabric/__init__.py
+Comment: 
+
+Filename: deep_training/nlp/rl/ppo_fabric/engine.py
+Comment: 
+
+Filename: deep_training/nlp/rl/ppo_fabric/ppo.py
+Comment: 
+
+Filename: deep_training/nlp/rl/ppo_fabric/ppo_agent.py
 Comment: 
 
 Filename: deep_training/nlp/scheduler/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/utils/__init__.py
 Comment: 
@@ -456,20 +468,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.3rc3.dist-info/METADATA
+Filename: deep_training-0.1.3rc4.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.3rc3.dist-info/WHEEL
+Filename: deep_training-0.1.3rc4.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.3rc3.dist-info/top_level.txt
+Filename: deep_training-0.1.3rc4.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.3rc3.dist-info/RECORD
+Filename: deep_training-0.1.3rc4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,23 +1,23 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.3rc3',
+    version='0.1.3rc4',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['pytorch-lightning>=2',
-                      'numpy-io>=0.0.1 , < 0.1.0',
+                      'numpy-io>=0.0.2 , < 0.1.0',
                       'sentencepiece',
                       'numpy',
                       'transformers >= 4.22',
                       'seqmetric',
                       'scipy',
                       'scikit-learn',
                       'tensorboard',
```

## deep_training/data_helper/data_helper.py

```diff
@@ -11,14 +11,15 @@
 from fastdatasets.torch_dataset import IterableDataset as torch_IterableDataset, Dataset as torch_Dataset
 from torch.utils.data import DataLoader, IterableDataset
 from .data_module import load_tokenizer, load_configure
 from .training_args import ModelArguments, DataArguments, TrainingArguments
 from ..utils.func import is_chinese_char
 from numpy_io.core.writer import DataWriteHelper
 from numpy_io.core.reader import load_numpy_dataset
+from numpy_io.pytorch_loader.dataloaders import load_distributed_random_sampler,load_random_sampler,load_sequential_sampler
 
 __all__ = [
     'DataHelper',
     'make_dataset',
     'is_chinese_char',
     'get_filename_no_ext',
     'get_filename_replace_dir',
@@ -81,33 +82,14 @@
                     if not line: continue
                     D.append(line)
         return D
 
 
 
 
-def check_dataset_file(files):
-    if not files:
-        return None
-
-    if isinstance(files, str):
-        if not os.path.exists(files):
-            return None
-    else:
-        #检测是否是文件list
-        files_ = [f for f in files if f is not None and isinstance(f, str) and os.path.exists(f)]
-        if not files_:
-            #检测是否是内存list
-            files = [f for f in files if f is not None and isinstance(f, list)]
-            if not files:
-                return None
-        else:
-            files = files_
-    return files
-
 class DataHelper(DataPreprocessHelper):
     def __init__(self,model_args: ModelArguments,
                 training_args: TrainingArguments,
                 data_args: DataArguments,**kwargs):
         super(DataHelper, self).__init__()
 
         self.backend = data_args.data_backend
@@ -234,19 +216,21 @@
                                   with_task_params=True,
                                   return_dict=False,
                                   with_print_labels=True,
                                   with_print_config=True,
                                   tokenizer_kwargs=None,
                                   config_kwargs=None):
 
-        if config_kwargs is None:
-            config_kwargs =  {}
         if tokenizer_kwargs is None:
             tokenizer_kwargs = {}
 
+        if config_kwargs is None:
+            config_kwargs = {}
+
+
         model_args: ModelArguments = self.model_args
         training_args: TrainingArguments = self.training_args
         data_args: DataArguments = self.data_args
 
 
 
         tokenizer = load_tokenizer(tokenizer_name=tokenizer_name or model_args.tokenizer_name,
@@ -314,210 +298,32 @@
 
 
         if with_labels:
             return tokenizer, config, self.label2id, self.id2label
         return tokenizer, config
 
 
-    """
-        cycle_length for IterableDataset
-        block_length for IterableDataset
-        return: 
-            torch DataLoader or fastdatasets numpy dataset
-    """
-    def load_dataset(self,files: typing.Union[typing.List, str],
-                     shuffle: bool=False,
-                     infinite: bool=False,
-                     cycle_length: int=4,
-                     block_length: int=10,
-                     num_processes: int = 1,
-                     process_index: int = 0,
-                     backend=None,
-                     with_record_iterable_dataset: bool = False,
-                     with_load_memory: bool = False,
-                     with_torchdataset: bool = True,
-                     transform_fn : typing.Callable = None,
-                     check_dataset_file_fn=None,
-                     limit_start: typing.Optional[int] = None,
-                     limit_count: typing.Optional[int] = None,
-                     dataset_loader_filter_fn: typing.Callable = None,
-                     ) -> typing.Optional[typing.Union[torch.utils.data.Dataset,torch.utils.data.IterableDataset]]:
-        assert process_index <= num_processes and num_processes >= 1
-        check_dataset_file_fn = check_dataset_file_fn or check_dataset_file
-        files = check_dataset_file_fn(files)
-        if files is None:
-            return None
-
-        dataset = load_numpy_dataset( files,
-                                      cycle_length=cycle_length,
-                                      block_length=block_length,
-                                      with_record_iterable_dataset=with_record_iterable_dataset,
-                                      with_parse_from_numpy=not with_load_memory,
-                                      backend=backend or self.backend,
-                                      limit_start=limit_start,
-                                      limit_count=limit_count,
-                                      dataset_loader_filter_fn=dataset_loader_filter_fn)
-        #加载至内存
-        if with_load_memory:
-            logging.info('load dataset to memory...')
-            if isinstance(dataset, typing.Iterator):
-                raw_data = [i for i in dataset]
-            else:
-                raw_data = [dataset[i] for i in range(len(dataset))]
-
-            dataset = MEMORY.load_dataset.SingleRandomDataset(raw_data)
-            #解析numpy数据
-            if self.backend != 'memory_raw':
-                dataset = dataset.parse_from_numpy_writer()
-
-
-        if isinstance(dataset, typing.Iterator):
-            dataset: IterableDatasetBase
-            if num_processes > 1:
-                dataset = dataset.mutiprocess(num_processes, process_index)
-
-            if shuffle:
-                dataset = dataset.shuffle(4096)
 
-            if infinite:
-                dataset = dataset.repeat(-1)
 
-            if transform_fn is not None:
-                dataset = dataset.map(transform_fn)
+    def load_distributed_random_sampler(self,*args,**kwargs):
+        if 'backend' not in kwargs:
+            kwargs.update({"backend": self.backend})
+        return load_distributed_random_sampler(*args,**kwargs)
+
+
+    def load_random_sampler(self,*args,**kwargs):
+        if 'backend' not in kwargs:
+            kwargs.update({"backend": self.backend})
+        return load_random_sampler(*args, **kwargs)
+
+    def load_sequential_sampler(self,*args,**kwargs):
+        if 'backend' not in kwargs:
+            kwargs.update({"backend": self.backend})
+        return load_sequential_sampler(*args, **kwargs)
 
-            dataset_ = torch_IterableDataset(dataset) if with_torchdataset else dataset
-        else:
-            dataset: RandomDatasetBase
-            if num_processes > 1:
-                dataset = dataset.mutiprocess(num_processes, process_index)
-
-            if shuffle:
-                dataset = dataset.shuffle(-1)
-
-            if transform_fn is not None:
-                dataset = dataset.map(transform_fn)
-
-            dataset_ = torch_Dataset(dataset) if with_torchdataset else dataset
-        return dataset_
-
-
-    def load_distributed_random_sampler(self,
-                                        files: typing.Union[typing.List, str],
-                                        batch_size,
-                                        num_processes: int = 1,
-                                        process_index: int = 0,
-                                        collate_fn=None,
-                                        pin_memory=False,
-                                        backend=None,
-                                        with_load_memory: bool = False,
-                                        with_torchdataset: bool = True,
-                                        transform_fn: typing.Callable = None,
-                                        check_dataset_file_fn=None,
-                                        limit_start: typing.Optional[int] = None,
-                                        limit_count: typing.Optional[int] = None,
-                                        dataset_loader_filter_fn: typing.Callable = None,
-                                        **kwargs
-                                        ):
-        dataset = self.load_dataset(
-            files, shuffle=False,
-            backend=backend, with_record_iterable_dataset=False,
-            with_load_memory=with_load_memory, with_torchdataset=with_torchdataset,
-            transform_fn=transform_fn, check_dataset_file_fn=check_dataset_file_fn,
-            limit_start=limit_start,
-            limit_count=limit_count,
-            dataset_loader_filter_fn=dataset_loader_filter_fn,
-        )
-        if dataset is None:
-            return None
-
-        sampler = torch.utils.data.distributed.DistributedSampler(dataset,num_replicas=num_processes,rank=process_index) if num_processes > 1 else None
-        return DataLoader(dataset, batch_size=batch_size,
-                          shuffle=sampler is None,
-                          sampler=sampler,
-                          collate_fn=collate_fn,
-                          pin_memory=pin_memory, **kwargs)
-
-    def load_random_sampler(self,files: typing.Union[typing.List, str],
-                     batch_size,
-                     collate_fn=None,
-                     pin_memory=False,
-                     shuffle: bool=False,
-                     infinite: bool=False,
-                     cycle_length: int=4,
-                     block_length: int=10,
-                     num_processes: int = 1,
-                     process_index: int = 0,
-                     backend=None,
-                     with_record_iterable_dataset: bool = False,
-                     with_load_memory: bool = False,
-                     with_torchdataset: bool = True,
-                     transform_fn : typing.Callable = None,
-                     check_dataset_file_fn=None,
-                    limit_start: typing.Optional[int] = None,
-                    limit_count: typing.Optional[int] = None,
-                    dataset_loader_filter_fn: typing.Callable = None,
-                    **kwargs
-                    ) -> typing.Optional[typing.Union[DataLoader,torch.utils.data.Dataset,torch.utils.data.IterableDataset,IterableDatasetBase,RandomDatasetBase]]:
-
-        dataset = self.load_dataset(
-            files,shuffle=shuffle,infinite=infinite,cycle_length=cycle_length,
-            block_length=block_length,num_processes=num_processes,process_index=process_index,
-            backend=backend,with_record_iterable_dataset=with_record_iterable_dataset,
-            with_load_memory=with_load_memory,with_torchdataset=with_torchdataset,
-            transform_fn=transform_fn,check_dataset_file_fn=check_dataset_file_fn,
-            limit_start=limit_start,
-            limit_count=limit_count,
-            dataset_loader_filter_fn=dataset_loader_filter_fn,
-        )
-        if dataset is None:
-            return None
-        return DataLoader(dataset,batch_size=batch_size,
-                          shuffle=False if isinstance(dataset, IterableDataset) else shuffle,
-                          collate_fn=collate_fn,
-                          pin_memory=pin_memory,**kwargs)
-
-    def load_sequential_sampler(self,files: typing.Union[typing.List, str],
-                     batch_size,
-                     collate_fn=None,
-                     pin_memory=False,
-                     shuffle: bool=False,
-                     infinite: bool=False,
-                     cycle_length: int=4,
-                     block_length: int=10,
-                     num_processes: int = 1,
-                     process_index: int = 0,
-                     backend=None,
-                     with_record_iterable_dataset: bool = False,
-                     with_load_memory: bool = False,
-                     with_torchdataset: bool = True,
-                     transform_fn : typing.Callable = None,
-                     check_dataset_file_fn=None,
-                    limit_start: typing.Optional[int] = None,
-                    limit_count: typing.Optional[int] = None,
-                    dataset_loader_filter_fn: typing.Callable = None,
-                    **kwargs
-                                ) -> typing.Optional[typing.Union[DataLoader,torch.utils.data.Dataset,torch.utils.data.IterableDataset,IterableDatasetBase,RandomDatasetBase]]:
-
-        dataset = self.load_dataset(
-            files,shuffle=shuffle,infinite=infinite,cycle_length=cycle_length,
-            block_length=block_length,num_processes=num_processes,process_index=process_index,
-            backend=backend,with_record_iterable_dataset=with_record_iterable_dataset,
-            with_load_memory=with_load_memory,with_torchdataset=with_torchdataset,
-            transform_fn=transform_fn,check_dataset_file_fn=check_dataset_file_fn,
-            limit_start=limit_start,
-            limit_count=limit_count,
-            dataset_loader_filter_fn=dataset_loader_filter_fn,
-        )
-        if dataset is None:
-            return None
-        return DataLoader(dataset,
-                          batch_size=batch_size,
-                          shuffle=shuffle,
-                          collate_fn=collate_fn,
-                          pin_memory=pin_memory,**kwargs)
 
     # 返回制作特征数据的中间文件
     def get_intermediate_file(self, intermediate_name, mode):
         data_args: DataArguments = self.data_args
         if data_args.data_backend.startswith('memory'):
             # 内存数据: list
             intermediate_output = []
```

## deep_training/nlp/layers/ppo.py

```diff
@@ -1,11 +1,10 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/23 15:13
-
-
+import numpy as np
 
 
 class AdaptiveKLController:
     """Adaptive KL Controller as described in Ziegler et al. "Fine-Tuning Language Models from Human Preferences"
     Reference: Section 2.2 https://arxiv.org/pdf/1909.08593.pdf#page=2
     Source: https://github.com/openai/lm-human-preferences/blob/master/lm_human_preferences/train_policy.py
     """
```

## deep_training/nlp/models/moss/quantization.py

```diff
@@ -1,32 +1,28 @@
 import numpy as np
 import torch
 import torch.nn as nn
 from torch.cuda.amp import custom_bwd, custom_fwd
 import math
+import triton
+import triton.language as tl
+from models.custom_autotune import *
 
 
 def find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):
     if type(module) in layers:
         return {name: module}
     res = {}
     for name1, child in module.named_children():
         res.update(find_layers(
             child, layers=layers, name=name + '.' + name1 if name != '' else name1
         ))
     return res
 
 
-try:
-    import triton
-    import triton.language as tl
-    from .custom_autotune import *
-except:
-    print('triton not installed. Run `pip install triton` to load quantized version of MOSS.')
-
 # code based https://github.com/fpgaminer/GPTQ-triton
 @autotune(
     configs=[
         triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},
                       num_stages=4, num_warps=4),
         triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},
                       num_stages=4, num_warps=4),
@@ -390,8 +386,8 @@
     model = model.eval()
     layers = find_layers(model)
     for name in ['lm_head']:
         if name in layers:
             del layers[name]
     make_quant(model, layers, wbits, groupsize)
     # model.load_state_dict(torch.load(checkpoint))
-    return model
+    return model
```

## Comparing `deep_training/nlp/models/rlhf/ppo/configuration.py` & `deep_training/nlp/rl/ppo/configuration.py`

 * *Files identical despite different names*

## Comparing `deep_training/nlp/models/rlhf/ppo/data_type.py` & `deep_training/nlp/rl/ppo/data_type.py`

 * *Files identical despite different names*

## Comparing `deep_training/nlp/models/rlhf/ppo/ppo.py` & `deep_training/nlp/rl/ppo/ppo.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 from typing import List, Callable, Tuple, Optional
 
 import torch
 from torch import nn
 from tqdm import tqdm
 from torch.nn import functional as F
 import torch.distributed as dist
-from .....nlp.layers.ppo import AdaptiveKLController, FixedKLController
+from ....nlp.layers.ppo import AdaptiveKLController, FixedKLController
 from .data_type import PPORLElement, PPORLBatch
 from .utils import logprobs_of_labels, Clock, gather_dict, RunningMoments, pad_across_processes, _gpu_gather, \
     get_tensor_stats, flatten_dict, whiten
 
 logger = logging.get_logger(__name__)
```

## Comparing `deep_training/nlp/models/rlhf/ppo/ppo_dataset.py` & `deep_training/nlp/rl/ppo/ppo_dataset.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from typing import List, Callable, Tuple, Optional
 
 import torch
 from torch import nn
 from tqdm import tqdm
 from torch.nn import functional as F
 import torch.distributed as dist
-from .....nlp.layers.ppo import AdaptiveKLController, FixedKLController
+from ....nlp.layers.ppo import AdaptiveKLController, FixedKLController
 from .data_type import PPORLElement, PPORLBatch
 from .utils import logprobs_of_labels, Clock, gather_dict, RunningMoments, pad_across_processes, _gpu_gather, \
     get_tensor_stats, flatten_dict, whiten
 
 logger = logging.get_logger(__name__)
 
 class PPO_dataset:
```

## Comparing `deep_training/nlp/models/rlhf/ppo/utils.py` & `deep_training/nlp/rl/ppo/utils.py`

 * *Files identical despite different names*

## Comparing `deep_training-0.1.3rc3.dist-info/METADATA` & `deep_training-0.1.3rc4.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-Metadata-Version: 2.1
-Name: deep-training
-Version: 0.1.3rc3
-Summary: an easy training architecture
-Home-page: https://github.com/ssbuild/deep_training
-Author: ssbuild
-Author-email: 9727464@qq.com
-License: Apache License 2.0
-Platform: UNKNOWN
-Requires-Dist: pytorch-lightning (>=2)
-Requires-Dist: numpy-io (<0.1.0,>=0.0.1)
-Requires-Dist: sentencepiece
-Requires-Dist: numpy
-Requires-Dist: transformers (>=4.22)
-Requires-Dist: seqmetric
-Requires-Dist: scipy
-Requires-Dist: scikit-learn
-Requires-Dist: tensorboard
-Requires-Dist: tqdm
-Requires-Dist: six
-
-torch_training: https://github.com/ssbuild/deep_training.git
-
-
+Metadata-Version: 2.1
+Name: deep-training
+Version: 0.1.3rc4
+Summary: an easy training architecture
+Home-page: https://github.com/ssbuild/deep_training
+Author: ssbuild
+Author-email: 9727464@qq.com
+License: Apache License 2.0
+Platform: UNKNOWN
+Requires-Dist: pytorch-lightning (>=2)
+Requires-Dist: numpy-io (<0.1.0,>=0.0.2)
+Requires-Dist: sentencepiece
+Requires-Dist: numpy
+Requires-Dist: transformers (>=4.22)
+Requires-Dist: seqmetric
+Requires-Dist: scipy
+Requires-Dist: scikit-learn
+Requires-Dist: tensorboard
+Requires-Dist: tqdm
+Requires-Dist: six
+
+torch_training: https://github.com/ssbuild/deep_training.git
+
+
```

## Comparing `deep_training-0.1.3rc3.dist-info/RECORD` & `deep_training-0.1.3rc4.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=gfyr4Z2_-VyytbJqC5Wua16Yzajs79o03jxtL95DR94,908
+deep_training/setup.py,sha256=RFPDUd1Qcqi2Mrg1VEns9b4qMn5gXBHH0W33tbeK8ME,908
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
-deep_training/data_helper/data_helper.py,sha256=ZMqfeoy42ULdUnhTjvyVRmuESw2qNiL2rA-1OF0on5Q,27373
+deep_training/data_helper/data_helper.py,sha256=kQEWL36NEhEoA7bw0zLA7CIYC_LYFtueXwr9deSthIM,17724
 deep_training/data_helper/data_module.py,sha256=rYsWFwRteIPqvPWYPXEKLcWB4S2Pq0bvlpznfe343qg,5082
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
 deep_training/nlp/layers/mask.py,sha256=8SB_Hl9X48-yuJMCPjLDabDXvgWvH4VPqUOSVDmePFs,435
 deep_training/nlp/layers/mhslayer.py,sha256=Ky6xW3hNe0x4WWPPoWa8pZkCp_-MR5VE0yKHYPzzpd0,1319
 deep_training/nlp/layers/norm.py,sha256=r_yHSnDAv8RY1nb1VS1lMWttbJVyCO376OIuu0So8c8,5911
-deep_training/nlp/layers/ppo.py,sha256=CCaGA5Jx47UnAUy7kBYcf4emPRt0498U9tdLaoAEO-A,1390
+deep_training/nlp/layers/ppo.py,sha256=wAz2eVJaXNYyepSHUOJSnAOUkJAdoG--bZ5EiRe_R_w,1406
 deep_training/nlp/layers/prefix_encoder.py,sha256=y_Y4wWLnEyZJf33pQJFjWfl-AX1VS8Aejj3JXOdiRXQ,1220
 deep_training/nlp/layers/seq_pointer.py,sha256=KmwZclK9gqdluy2o-h7nd3zL3EZjjjw1L6dz0isCFI8,7259
 deep_training/nlp/layers/w2ner.py,sha256=fP7hlMHp1NTH6elNMJA-wBOER76VTouSSsKCinLsCyM,3550
 deep_training/nlp/layers/lora_v1/__init__.py,sha256=gmwJqLmiKqPfh5_VGWWr38y8lLgdPWw6JrjNVrvsLEY,72
 deep_training/nlp/layers/lora_v1/layers.py,sha256=JWz9RtqA-hLsTxyhZPBlz82aN_VaPjNoDYRhssKV1H0,15095
 deep_training/nlp/layers/lora_v1/utils.py,sha256=1ouFUmTF9IXzum97eIlrTeT6J4OAnEwIaWkZdgXMjSc,1819
 deep_training/nlp/layers/lora_v2/__init__.py,sha256=dGpWUx0v7UoVgwZY5srCDCBvt_hlI77zA6mQO3CxMaE,72
@@ -112,32 +112,36 @@
 deep_training/nlp/models/lora/v2/lora_model.py,sha256=5k09kzj8bSvU-CQm5GJWtg5isXUEUYPmvKuxdLPc_V4,11745
 deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=tzDCWUiGYC81cbfNiu4ZKo3VZsftM_SF8NwH8r56BO0,10265
 deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
 deep_training/nlp/models/moss/__init__.py,sha256=_dQslDggRX8ZsR6RPTUuBzAHotQt8MPAqZ1-nGULPns,467
 deep_training/nlp/models/moss/configuration_moss.py,sha256=Qqp7anpWGnsotkqd5UOfc9e5zhxgx7j5xetSQUOmhaQ,5097
 deep_training/nlp/models/moss/custom_autotune.py,sha256=O-C9w-hZkcrUgDfK4B1iPtKjHQAZwELNefYhlLABHyc,6735
 deep_training/nlp/models/moss/modeling_moss.py,sha256=Trm6zkUFQo9SkgInIuusSFekRvyLa6x2W6WyRE9CswI,31079
-deep_training/nlp/models/moss/quantization.py,sha256=z43YME9DW_yER96uUoyWdE0GwyZaZX4psA40LRHLhP0,18866
+deep_training/nlp/models/moss/quantization.py,sha256=AWHuzdahL7uYwTCcSpXOqD4o2g-q-es_s1nStOCkfQ8,18750
 deep_training/nlp/models/moss/tokenization_moss.py,sha256=Ft7hwLBfYoAqn33anM0sbkvU7GuXJQW8NJ1Ddko_1hk,15939
-deep_training/nlp/models/rlhf/__init__.py,sha256=c1W6T8PhNnCVnEIKtr7qu-jHY2IliW9HcMOv5TZ92b0,55
-deep_training/nlp/models/rlhf/ppo/__init__.py,sha256=IqNQicmSmtZVbJIdNZdaQxpx0EbqvTJSUb2Bx1pRdys,55
-deep_training/nlp/models/rlhf/ppo/configuration.py,sha256=RjPR9mOa-sxtgD65dCfI5os_L7XADs591fJrxNBdodY,2238
-deep_training/nlp/models/rlhf/ppo/data_type.py,sha256=YSz3BPegvAC6Pl3x-dnFRbBV-qSxiJjwVwpUvbmMV7U,2691
-deep_training/nlp/models/rlhf/ppo/ppo.py,sha256=awBy3E1Yftz8kBUa46ojwIxy1t1L1FlVaegLF9zKJ5I,8064
-deep_training/nlp/models/rlhf/ppo/ppo_dataset.py,sha256=ETmwxeaWEOaReDozOrRax4RH2bA-PaPZdyQQ5W-DTnI,16349
-deep_training/nlp/models/rlhf/ppo/utils.py,sha256=9oDHyKiXKXJDpLuVfUyUaA6FhyobLmaU7seY_hupPYU,10496
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
 deep_training/nlp/models/splinker/splinker.py,sha256=AhIWyfUtNOLqwZn520J-mv8LJwIoDZpo8yNoc4V5Gss,2866
 deep_training/nlp/models/t5decoder/__init__.py,sha256=R9Op4Ysli9isootQQ2FcjhpbG13fNESlmUROu6cfGH0,14478
 deep_training/nlp/models/t5encoder/__init__.py,sha256=692ChfLf2sZWgzhBM37g1PdpmEmsU1R9RRl_uTHRET0,6646
 deep_training/nlp/optimizer/__init__.py,sha256=c4cmx9ebIdqwXBu3N9QbcNNHb32t2MV6fTK9aC2VBGQ,56
 deep_training/nlp/optimizer/lamb.py,sha256=htvZQHPWHG5GCDgo9xCaZikWwRyaD2PjDioIQvX7qXw,5225
 deep_training/nlp/optimizer/lion/__init__.py,sha256=AvYkLp7sOpRIC3a5ejuniUUKyQmmBA1TPJdt2RA7Nqg,99
 deep_training/nlp/optimizer/lion/lion.py,sha256=kDN4_dz1N6G0q8PhQBpdQRNhtm5MXNSx-kde2JP2FlU,2295
 deep_training/nlp/optimizer/lion/triton.py,sha256=QLdWMW7cgqQU6Zb8uqmESONr7S7Nf1Tk4TR7OwACDQo,2198
+deep_training/nlp/rl/__init__.py,sha256=lXJsb8d-9R7DshCEdcx3iPyndlf1t5FNXiJsh1SUr0s,79
+deep_training/nlp/rl/ppo/__init__.py,sha256=IqNQicmSmtZVbJIdNZdaQxpx0EbqvTJSUb2Bx1pRdys,55
+deep_training/nlp/rl/ppo/configuration.py,sha256=RjPR9mOa-sxtgD65dCfI5os_L7XADs591fJrxNBdodY,2238
+deep_training/nlp/rl/ppo/data_type.py,sha256=YSz3BPegvAC6Pl3x-dnFRbBV-qSxiJjwVwpUvbmMV7U,2691
+deep_training/nlp/rl/ppo/ppo.py,sha256=rJ8edj86BF5XgVcw9sZMLM629ixElHJXNLMmHrUTDko,8063
+deep_training/nlp/rl/ppo/ppo_dataset.py,sha256=IG96gmT0gD_Icw2mCdw_2OxZrKyVi_5QsoU2dQq79lA,16348
+deep_training/nlp/rl/ppo/utils.py,sha256=9oDHyKiXKXJDpLuVfUyUaA6FhyobLmaU7seY_hupPYU,10496
+deep_training/nlp/rl/ppo_fabric/__init__.py,sha256=t568ZV7B9LwcorfgWdMWOHkoxo7cZJam7lBpzVBD6HY,54
+deep_training/nlp/rl/ppo_fabric/engine.py,sha256=G8GqioYmcFzuV1patyw9lDbX4wTs0hIlSWC8CLPXCt4,8621
+deep_training/nlp/rl/ppo_fabric/ppo.py,sha256=ixg_l8FPFtKliwEgmFUc01IhxQ3Pf_jkvYfNOVL_P14,5736
+deep_training/nlp/rl/ppo_fabric/ppo_agent.py,sha256=hBjLtVI8f8o5BJD7TQUJS4vJ0SL8mekh9bk6uMTjH-Q,5977
 deep_training/nlp/scheduler/__init__.py,sha256=-zaiinwJzOBWypkNodSZO12kqbswVsPy5JCsYpvLbbY,2868
 deep_training/nlp/utils/__init__.py,sha256=DF-_NzNu7n93AwN-hJcae38k5FbtqvxDKh62y8S_Opo,6986
 deep_training/nlp/utils/adversarial.py,sha256=FNZlg8mV23YXRu7aDcu1JZBUGBV01hi_bwRzfFyzEzM,6323
 deep_training/nlp/utils/nlputils.py,sha256=KEmFliU1IqJHy3INNDvOriEMlBkP8GNwe8Y8_c_imZQ,15256
 deep_training/nlp/utils/spearman.py,sha256=tOpaah5bt_65ferL_uI6FMfKvNexi7CQztSYLj-k3yo,795
 deep_training/tfnlp/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
 deep_training/tfnlp/layers/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
@@ -148,11 +152,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=kbojvTgVcYX8bJ141J7VS14-JrD480Oh33P_tnwjlks,7430
-deep_training-0.1.3rc3.dist-info/METADATA,sha256=MkfGpt_k9KoY4B3YlFmiqSFleflfqJOprmLMrHfwP_U,637
-deep_training-0.1.3rc3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-deep_training-0.1.3rc3.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.3rc3.dist-info/RECORD,,
+deep_training-0.1.3rc4.dist-info/METADATA,sha256=PpUbxIwVWByoeImrCqLUek1rIgdPO-AI9e4Dw8hx420,613
+deep_training-0.1.3rc4.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.3rc4.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.3rc4.dist-info/RECORD,,
```

