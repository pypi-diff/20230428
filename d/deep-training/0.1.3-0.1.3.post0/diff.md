# Comparing `tmp/deep_training-0.1.3-py3-none-any.whl.zip` & `tmp/deep_training-0.1.3.post0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,14 +1,14 @@
-Zip file size: 284236 bytes, number of entries: 162
+Zip file size: 284310 bytes, number of entries: 162
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      905 b- defN 23-Apr-28 05:45 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      911 b- defN 23-Apr-28 07:50 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
 -rw-rw-rw-  2.0 fat    17724 b- defN 23-Apr-28 00:24 deep_training/data_helper/data_helper.py
--rw-rw-rw-  2.0 fat     5082 b- defN 23-Apr-27 00:33 deep_training/data_helper/data_module.py
+-rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-28 06:13 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
 -rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
@@ -84,20 +84,20 @@
 -rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
 -rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
 -rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
 -rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
 -rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
 -rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
 -rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26308 b- defN 23-Apr-27 00:33 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat    26351 b- defN 23-Apr-28 07:51 deep_training/nlp/models/transformer_base.py
 -rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
 -rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
--rw-rw-rw-  2.0 fat    16500 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA/__init__.py
+-rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
--rw-rw-rw-  2.0 fat    19183 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA_parallel/__init__.py
+-rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
 -rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
 -rw-rw-rw-  2.0 fat    60508 b- defN 23-Apr-19 01:01 deep_training/nlp/models/chatglm/__init__.py
 -rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
 -rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
 -rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-17 00:24 deep_training/nlp/models/chatglm/tokenization.py
@@ -153,12 +153,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat     7430 b- defN 23-Apr-27 00:33 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      610 b- defN 23-Apr-28 05:46 deep_training-0.1.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-28 05:46 deep_training-0.1.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-28 05:46 deep_training-0.1.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    15640 b- defN 23-Apr-28 05:46 deep_training-0.1.3.dist-info/RECORD
-162 files, 964351 bytes uncompressed, 258906 bytes compressed:  73.2%
+-rw-rw-rw-  2.0 fat      616 b- defN 23-Apr-28 07:52 deep_training-0.1.3.post0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-28 07:52 deep_training-0.1.3.post0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-28 07:52 deep_training-0.1.3.post0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    15664 b- defN 23-Apr-28 07:52 deep_training-0.1.3.post0.dist-info/RECORD
+162 files, 964437 bytes uncompressed, 258932 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -468,20 +468,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.3.dist-info/METADATA
+Filename: deep_training-0.1.3.post0.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.3.dist-info/WHEEL
+Filename: deep_training-0.1.3.post0.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.3.dist-info/top_level.txt
+Filename: deep_training-0.1.3.post0.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.3.dist-info/RECORD
+Filename: deep_training-0.1.3.post0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,15 +1,15 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.3',
+    version='0.1.3@post0',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['pytorch-lightning>=2',
```

## deep_training/data_helper/data_module.py

```diff
@@ -15,15 +15,14 @@
                    do_lower_case=None,
                    use_fast_tokenizer=True,
                    model_revision="main",
                    use_auth_token=None,
                    **kwargs):
     tokenizer_kwargs = {
         "cache_dir": cache_dir,
-        "do_lower_case": do_lower_case,
         "revision": model_revision,
         "use_auth_token": True if use_auth_token else None,
         **kwargs
     }
     if do_lower_case is not None:
         tokenizer_kwargs['do_lower_case'] = do_lower_case
```

## deep_training/nlp/models/transformer_base.py

```diff
@@ -2,14 +2,15 @@
 # @Time    : 2023/4/11 14:35
 
 import sys
 import typing
 from functools import partial
 from typing import Any, IO
 
+import lightning
 import lightning as pl
 import torch
 from torch import nn, Tensor
 from transformers import (
     PretrainedConfig,
 )
 
@@ -272,15 +273,16 @@
 
             self.training_step = self.adv_training_step
             if training_args.adv['mode'].find('local') != -1:
                 self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model)
             else:
                 self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model,
                                                                                  emb_name=training_args.adv.get('emb_name', 'embedding'))
-            k = 'lightning.trainer.configuration_validator'
+
+            k = 'lightning.pytorch.trainer.configuration_validator'
             if k in sys.modules:
                 setattr( sys.modules[k],'__verify_manual_optimization_support' , verify_manual_optimization_support)
         else:
             self.adversarial = None
 
         self.gradient_clip_val = training_args.max_grad_norm
 
@@ -303,15 +305,15 @@
                 return embeddings
 
             position_embeddings = self.get_embeddings_module().embeddings.position_embeddings
             position_embeddings.forward = partial(forward,position_embeddings)
 
 
     def get_embeddings_module(self):
-        base_model_prefix = self.backbone.base_model_prefix
+        base_model_prefix = self.backbone._premodel_data.base_model_prefix
         current_model = self.backbone.model
         tmp_obj = current_model
         while tmp_obj is not None:
             if hasattr(tmp_obj, 'embeddings'):
                 return tmp_obj
             current_model = tmp_obj
             tmp_obj = getattr(current_model, base_model_prefix, None)
```

## deep_training/nlp/models/LLaMA/__init__.py

```diff
@@ -305,17 +305,15 @@
         )
 
         self.layers = torch.nn.ModuleList()
         for layer_id in range(config.n_layer):
             self.layers.append(LLaMABlock(layer_id, config))
 
         self.norm = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)
-        self.output = nn.Linear(
-            config.hidden_size, config.vocab_size, bias=False,
-        )
+
 
         self.freqs_cis = precompute_freqs_cis(
             self.config.hidden_size // self.config.n_head, self.config.max_seq_len * 2
         )
         
         self.post_init()
 
@@ -336,28 +334,32 @@
             mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=input_ids.device)
             mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)
 
         for layer in self.layers:
             h = layer(h, start_pos, freqs_cis, mask)
         h = self.norm(h)
 
-        if self.config.inference:
-            h = self.output(h[:, -1]).float()
-        else:
-            h = self.output(h)
-        return (h,)
+        # if self.config.inference:
+        #     h = self.output(h[:, -1]).float()
+        # else:
+        #     h = self.output(h)
+        return h
 
 
 class LLaMALMHeadModel(LLaMAPreTrainedModel):
     _keys_to_ignore_on_load_missing = [r"attn.masked_bias", r"attn.bias", r"lm_head.weight"]
 
     def __init__(self, config):
         super(LLaMALMHeadModel, self).__init__(config)
         self.transformer = LLaMAModel(config)
 
+        self.lm_head = nn.Linear(
+            config.hidden_size, config.vocab_size, bias=False,
+        )
+
         self.model_parallel = False
         self.device_map = None
         # Initialize weights and apply final processing
         self.post_init()
 
     def get_output_embeddings(self):
         return self.lm_head
@@ -395,15 +397,15 @@
         hidden_states = transformer_outputs
 
         # Set device for model parallelism
         if self.model_parallel:
             torch.cuda.set_device(self.transformer.first_device)
             hidden_states = hidden_states.to(self.lm_head.weight.device)
 
-        lm_logits = hidden_states
+        lm_logits = self.lm_head(hidden_states)
         loss = None
         if labels is not None:
             # Shift so that tokens < n predict n
             shift_logits = lm_logits[..., :-1, :].contiguous()
             shift_labels = labels[..., 1:].contiguous()
             # Flatten the tokens
             loss_fct = CrossEntropyLoss()
```

## deep_training/nlp/models/LLaMA_parallel/__init__.py

```diff
@@ -362,17 +362,15 @@
 
         self.layers = torch.nn.ModuleList()
         for layer_id in range(config.n_layer):
             self.layers.append(LLaMABlock(layer_id, config))
 
         self.norm = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)
 
-        self.output = ColumnParallelLinear(
-            config.n_embd, config.vocab_size, bias=False, init_method=lambda x: x
-        )
+
 
         self.freqs_cis = precompute_freqs_cis(
             self.config.hidden_size // self.config.n_head, self.config.max_seq_len * 2
         )
 
 
         self.post_init()
@@ -405,30 +403,34 @@
             mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=input_ids.device)
             mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)
 
         for layer in self.layers:
             h = layer(h, start_pos, freqs_cis, mask)
         h = self.norm(h)
 
-        if self.config.inference:
-            h = self.output(h[:,-1]).float()
-        else:
-            h = self.output(h)
-        return (h,)
+        # if self.config.inference:
+        #     h = self.output(h[:,-1]).float()
+        # else:
+        #     h = self.output(h)
+        return h
 
 
 
 
 class LLaMALMHeadModel(LLaMAPreTrainedModel):
     _keys_to_ignore_on_load_missing = [r"attn.masked_bias", r"attn.bias", r"lm_head.weight"]
 
     def __init__(self, config):
         super(LLaMALMHeadModel, self).__init__(config)
         self.transformer = LLaMAModel(config)
 
+        self.lm_head = ColumnParallelLinear(
+            config.n_embd, config.vocab_size, bias=False, init_method=lambda x: x
+        )
+
         self.model_parallel = False
         self.device_map = None
         # Initialize weights and apply final processing
         self.post_init()
 
     def get_output_embeddings(self):
         return self.lm_head
@@ -465,15 +467,15 @@
         )
         hidden_states = transformer_outputs[0]
 
         # Set device for model parallelism
         if self.model_parallel:
             torch.cuda.set_device(self.transformer.first_device)
             hidden_states = hidden_states.to(self.lm_head.weight.device)
-        lm_logits = hidden_states
+        lm_logits = self.lm_head(hidden_states)
         loss = None
         if labels is not None:
 
             # Shift so that tokens < n predict n
             shift_logits = lm_logits[..., :-1, :].contiguous()
             shift_labels = labels[..., 1:].contiguous()
             # Flatten the tokens
```

## Comparing `deep_training-0.1.3.dist-info/METADATA` & `deep_training-0.1.3.post0.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.3
+Version: 0.1.3-post0
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: pytorch-lightning (>=2)
```

## Comparing `deep_training-0.1.3.dist-info/RECORD` & `deep_training-0.1.3.post0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=TZOJ8TIkhxqnuXXmdkYkIeCdbcBJEWSyojDdBpUA_BY,905
+deep_training/setup.py,sha256=Oy6XiX-KQ2IYmPLecOxz2nojUbAb7ELvJqoLHNhwegY,911
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
 deep_training/data_helper/data_helper.py,sha256=kQEWL36NEhEoA7bw0zLA7CIYC_LYFtueXwr9deSthIM,17724
-deep_training/data_helper/data_module.py,sha256=rYsWFwRteIPqvPWYPXEKLcWB4S2Pq0bvlpznfe343qg,5082
+deep_training/data_helper/data_module.py,sha256=EmXCTU2jLnldgHubQL4lpwzmlJErSVJf7YIotbQBQJU,5041
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
 deep_training/nlp/layers/mask.py,sha256=8SB_Hl9X48-yuJMCPjLDabDXvgWvH4VPqUOSVDmePFs,435
@@ -83,20 +83,20 @@
 deep_training/nlp/models/pure_model.py,sha256=LD8cYvvRirnP8iMFCyRhsNXRHpZt93Kh2WGoUZoEnFw,5149
 deep_training/nlp/models/simcse.py,sha256=ubVGkeMatDeIUqySV8Tc2TJHvaRKb4p3JOvUTOOhaRo,3949
 deep_training/nlp/models/span_ner.py,sha256=rD0TY2K-zesfRFGfDguqkSAfxHGgbQHG3K5QZ6gc7Zg,6022
 deep_training/nlp/models/spn4re.py,sha256=g_pk3bNqpH41EnzJ77oWPsKvbUwzJfaBYMux5FiEc60,14454
 deep_training/nlp/models/tplinker.py,sha256=PJ9smipeIiA1CDi8xz1gIg2DBWzO5C1B2wITItEsd1A,11383
 deep_training/nlp/models/tplinkerplus.py,sha256=hP4KD3rf2hktfQzHnI7RA2j_2cjk_0G5v6CkbLt1gvQ,8157
 deep_training/nlp/models/transformer.py,sha256=ZuywgLt3HZhsR4sJ4SyZvrYgaVJW8lCn5JH1j_IceXE,6624
-deep_training/nlp/models/transformer_base.py,sha256=RUJU3bzd7UF007j3HuYyrxKusx6d_5ltYO23XFJadTA,26308
+deep_training/nlp/models/transformer_base.py,sha256=fVX5zYqR19npxet9G6utAaMdHxXLdAEjsGVbAmPdPLE,26351
 deep_training/nlp/models/tsdae_model.py,sha256=lb04RIGkhHhilD-vdkfb8YK9hnTck1nN79WX1Pngbbk,7968
 deep_training/nlp/models/w2ner.py,sha256=z0BortOquZSzmma355wNLz1ofLku_hMb2CjL4KDf-PM,9040
-deep_training/nlp/models/LLaMA/__init__.py,sha256=n_M2atEv-G2i3oy_YkLCVu5QiLLyfxEj2xr91h1dWEw,16500
+deep_training/nlp/models/LLaMA/__init__.py,sha256=asn9Wxkl4lG12dRVgxq7Lz4BGCNDaRFL155haVMDNso,16524
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
-deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=5lsrh09pBTzRRKwc1bJtJs37Qn5qKqif_5S0pOoD1zc,19183
+deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=4fOhbq0tQOTSH5e3X6XN3PnI6athUR8tsTCn4AUg94Q,19207
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
 deep_training/nlp/models/PaLM/configuration.py,sha256=kIb3nj-2pQB2wyNrYHSZqr_ta1F0Cg-VbGEbnM5icPc,5890
 deep_training/nlp/models/chatglm/__init__.py,sha256=t9u927aSHY0mVpxtGrY7_jMEx25g9rsHKcE0HR3GWGc,60508
 deep_training/nlp/models/chatglm/configuration.py,sha256=4w-Kbp_FJ2crIQVyu6kie9lbMSuE3U4nnjwjVPos2E8,4575
 deep_training/nlp/models/chatglm/quantization.py,sha256=sqX_poTcYNLJLDPbCwfRllDCF0enhshjX_dw7yZa604,15150
 deep_training/nlp/models/chatglm/tokenization.py,sha256=XPdz8DFekZ3qsYakyP2iZbk0Iu1cCXgnXjrBKHDO_s8,16642
@@ -152,11 +152,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=kbojvTgVcYX8bJ141J7VS14-JrD480Oh33P_tnwjlks,7430
-deep_training-0.1.3.dist-info/METADATA,sha256=JEzP6x1F1nJUnbOfSxLMWCnLcRELXT7vh65StH0ASls,610
-deep_training-0.1.3.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.3.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.3.dist-info/RECORD,,
+deep_training-0.1.3.post0.dist-info/METADATA,sha256=x6v-GJBbK9mt36luneilSoLo2GVcmYmCy8ClK-moABM,616
+deep_training-0.1.3.post0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.3.post0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.3.post0.dist-info/RECORD,,
```

